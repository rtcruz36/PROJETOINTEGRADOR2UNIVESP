diff --git a/apps/analytics/serializers.py b/apps/analytics/serializers.py
index c3f68667191e5dda41a47be86e964ba3c9f88518..357a951d24adffa7766a1a20325550078f75ef13 100644
--- a/apps/analytics/serializers.py
+++ b/apps/analytics/serializers.py
@@ -1,23 +1,120 @@
 # apps/analytics/serializers.py
 
 from rest_framework import serializers
 
+
 class TopicCorrelationSerializer(serializers.Serializer):
-    """
-    Serializa os dados de um único tópico para a análise de correlação.
-    """
+    """Serializa os dados de um único tópico para a análise de correlação."""
+
     topic_id = serializers.IntegerField()
     topic_title = serializers.CharField()
     total_minutes_studied = serializers.IntegerField()
     average_quiz_score = serializers.FloatField()
 
+
 class CorrelationAnalyticsSerializer(serializers.Serializer):
-    """
-    Serializa o resultado final da análise de correlação.
-    """
-    correlation_coefficient = serializers.FloatField(allow_null=True, help_text="Coeficiente de correlação de Pearson (-1 a 1). Null se não for calculável.")
-    interpretation = serializers.CharField(help_text="Interpretação em linguagem natural do coeficiente.")
-    data_points = serializers.IntegerField(help_text="Número de tópicos com dados suficientes para o cálculo.")
-    # Mostra os dados brutos que foram usados no cálculo, o que é ótimo para debug e para exibir em gráficos.
+    """Serializa o resultado final da análise de correlação."""
+
+    correlation_coefficient = serializers.FloatField(
+        allow_null=True,
+        help_text="Coeficiente de correlação de Pearson (-1 a 1). Null se não for calculável.",
+    )
+    interpretation = serializers.CharField(
+        help_text="Interpretação em linguagem natural do coeficiente."
+    )
+    data_points = serializers.IntegerField(
+        help_text="Número de tópicos com dados suficientes para o cálculo."
+    )
     topic_data = TopicCorrelationSerializer(many=True)
 
+
+class ScoreTimelineEntrySerializer(serializers.Serializer):
+    """Representa a média de notas em uma data específica."""
+
+    date = serializers.DateField()
+    average_score = serializers.FloatField()
+    attempt_count = serializers.IntegerField()
+
+
+class TopicProgressSerializer(serializers.Serializer):
+    """Detalhes de evolução de notas para um tópico."""
+
+    topic_id = serializers.IntegerField()
+    topic_title = serializers.CharField()
+    course_title = serializers.CharField()
+    attempt_count = serializers.IntegerField()
+    latest_score = serializers.FloatField()
+    score_change = serializers.FloatField()
+
+
+class ScoreProgressSerializer(serializers.Serializer):
+    """Serializa a progressão temporal das notas."""
+
+    total_attempts = serializers.IntegerField()
+    trend_summary = serializers.CharField()
+    timeline = ScoreTimelineEntrySerializer(many=True)
+    per_topic = TopicProgressSerializer(many=True)
+
+
+class TopicComparisonEntrySerializer(serializers.Serializer):
+    """Informações agregadas por tópico."""
+
+    topic_id = serializers.IntegerField()
+    topic_title = serializers.CharField()
+    course_title = serializers.CharField()
+    total_minutes = serializers.IntegerField()
+    session_count = serializers.IntegerField()
+    average_score = serializers.FloatField(allow_null=True)
+    attempt_count = serializers.IntegerField()
+
+
+class CourseComparisonEntrySerializer(serializers.Serializer):
+    """Informações agregadas por disciplina."""
+
+    course_id = serializers.IntegerField()
+    course_title = serializers.CharField()
+    total_minutes = serializers.IntegerField()
+    session_count = serializers.IntegerField()
+    average_score = serializers.FloatField(allow_null=True)
+    attempt_count = serializers.IntegerField()
+
+
+class TopicComparisonSerializer(serializers.Serializer):
+    """Comparação de dedicação e desempenho entre tópicos e cursos."""
+
+    by_topic = TopicComparisonEntrySerializer(many=True)
+    by_course = CourseComparisonEntrySerializer(many=True)
+    summary = serializers.CharField()
+
+
+class WeeklyMinutesSerializer(serializers.Serializer):
+    """Quantidade de minutos estudados por semana."""
+
+    week_start = serializers.DateField()
+    week_end = serializers.DateField()
+    total_minutes = serializers.IntegerField()
+
+
+class EngagementMetricsSerializer(serializers.Serializer):
+    """Serializa métricas de engajamento do usuário."""
+
+    current_streak = serializers.IntegerField()
+    best_streak = serializers.IntegerField()
+    total_minutes_last_7_days = serializers.IntegerField()
+    total_minutes_last_30_days = serializers.IntegerField()
+    average_session_minutes = serializers.FloatField(allow_null=True)
+    sessions_last_7_days = serializers.IntegerField()
+    regularity_score = serializers.FloatField()
+    most_productive_day = serializers.CharField(allow_null=True)
+    weekly_minutes = WeeklyMinutesSerializer(many=True)
+    summary = serializers.CharField()
+
+
+class AnalyticsDashboardSerializer(serializers.Serializer):
+    """Serializer agregado para o painel completo de analytics."""
+
+    study_effectiveness = CorrelationAnalyticsSerializer()
+    score_progression = ScoreProgressSerializer()
+    topic_comparison = TopicComparisonSerializer()
+    engagement_metrics = EngagementMetricsSerializer()

diff --git a/apps/analytics/tests.py b/apps/analytics/tests.py
index 3b6f4c1c3a9820934d4956080c4b863c0f24e639..8249f29b8b428c1d69913666275ab4eff95daf70 100644
--- a/apps/analytics/tests.py
+++ b/apps/analytics/tests.py
@@ -1,33 +1,35 @@
 # apps/analytics/tests.py
 
 import datetime
 from django.urls import reverse
 from rest_framework import status
 from rest_framework.test import APITestCase,APIClient
 import numpy as np # Usaremos numpy para o cálculo da correlação no teste
 from django.test import SimpleTestCase
+from django.utils import timezone
+
 from apps.analytics.views import get_correlation_interpretation
 
 from apps.accounts.models import User
 from apps.learning.models import Course, Topic
 from apps.scheduling.models import StudyLog
 from apps.assessment.models import Quiz, Attempt
 from unittest.mock import patch, MagicMock
 from django.contrib.auth import get_user_model
 
 
 User = get_user_model()
 
 class StudyEffectivenessViewEdgeCases(APITestCase):
     def setUp(self):
         self.user = User.objects.create_user(username="u", password="p")
         self.client = APIClient()
         self.client.force_authenticate(self.user)
 
     @patch("apps.analytics.views.Topic.objects")
     @patch("apps.analytics.views.pearsonr")
     def test_pearsonr_exception_results_in_none(self, mock_pearsonr, mock_topic_objects):
         # Arrange: mock do queryset para produzir analysis_data com >= 2 pontos
         class MockTopic:
             def __init__(self, i, title, mins, score=None):
                 self.id = i
@@ -280,26 +282,132 @@ class AnalyticsAPITests(APITestCase):
         topic_sem_quiz = Topic.objects.create(course=course, title="Tópico Sem Quiz")
         StudyLog.objects.create(user=self.user, course=course, topic=topic_sem_quiz, date=datetime.date.today(), minutes_studied=100)
 
         # Tópico 4: Tem quiz, mas não tem log de estudo
         topic_sem_estudo = Topic.objects.create(course=course, title="Tópico Sem Estudo")
         quiz = Quiz.objects.create(topic=topic_sem_estudo, title="Quiz Inútil")
         Attempt.objects.create(user=self.user, quiz=quiz, score=50.0)
 
         # Ação
         response = self.client.get(self.url)
 
         # Verificação
         self.assertEqual(response.status_code, status.HTTP_200_OK)
         
         data = response.data
         # Apenas 2 dos 4 tópicos devem ser usados no cálculo
         self.assertEqual(data['data_points'], 2)
         self.assertIsNotNone(data['correlation_coefficient'])
         self.assertEqual(len(data['topic_data']), 2)
         
         # Verifica se os tópicos corretos foram usados
         included_topics = [d['topic_title'] for d in data['topic_data']]
         self.assertIn("Permutações", included_topics)
         self.assertIn("Combinações", included_topics)
         self.assertNotIn("Tópico Sem Quiz", included_topics)
-        self.assertNotIn("Tópico Sem Estudo", included_topics)
\ No newline at end of file
+        self.assertNotIn("Tópico Sem Estudo", included_topics)
+
+
+class AdditionalAnalyticsEndpointsTests(APITestCase):
+    """Testes para as novas análises disponíveis."""
+
+    def setUp(self):
+        self.user = User.objects.create_user(
+            username='dash', email='dash@example.com', password='password'
+        )
+        self.client.force_authenticate(self.user)
+        self.course = Course.objects.create(user=self.user, title='História')
+        self.topic_a = Topic.objects.create(course=self.course, title='Idade Média')
+        self.topic_b = Topic.objects.create(course=self.course, title='Idade Moderna')
+
+        today = timezone.localdate()
+
+        # Criar logs consecutivos para formar streaks e métricas temporais
+        for offset, minutes in enumerate([45, 60, 30, 90]):
+            StudyLog.objects.create(
+                user=self.user,
+                course=self.course,
+                topic=self.topic_a,
+                date=today - datetime.timedelta(days=offset),
+                minutes_studied=minutes,
+            )
+
+        StudyLog.objects.create(
+            user=self.user,
+            course=self.course,
+            topic=self.topic_b,
+            date=today - datetime.timedelta(days=10),
+            minutes_studied=120,
+        )
+
+        quiz_a = Quiz.objects.create(topic=self.topic_a, title='Quiz A')
+        quiz_b = Quiz.objects.create(topic=self.topic_b, title='Quiz B')
+
+        attempt_dates = [
+            timezone.now() - datetime.timedelta(days=5),
+            timezone.now() - datetime.timedelta(days=2),
+            timezone.now() - datetime.timedelta(days=1),
+        ]
+        scores = [55.0, 70.0, 82.0]
+        for attempt_date, score in zip(attempt_dates, scores):
+            attempt = Attempt.objects.create(
+                user=self.user,
+                quiz=quiz_a,
+                score=score,
+            )
+            Attempt.objects.filter(id=attempt.id).update(completed_at=attempt_date)
+
+        attempt_b = Attempt.objects.create(
+            user=self.user,
+            quiz=quiz_b,
+            score=88.0,
+        )
+        Attempt.objects.filter(id=attempt_b.id).update(
+            completed_at=timezone.now() - datetime.timedelta(days=3)
+        )
+
+    def test_study_progress_endpoint(self):
+        response = self.client.get(reverse('analytics-study-progress'))
+        self.assertEqual(response.status_code, status.HTTP_200_OK)
+        data = response.data
+
+        self.assertEqual(data['total_attempts'], 4)
+        self.assertGreaterEqual(len(data['timeline']), 1)
+        self.assertGreaterEqual(len(data['per_topic']), 2)
+        self.assertIn('trend_summary', data)
+
+        topic_titles = {entry['topic_title'] for entry in data['per_topic']}
+        self.assertIn('Idade Média', topic_titles)
+        self.assertIn('Idade Moderna', topic_titles)
+
+    def test_topic_comparison_endpoint(self):
+        response = self.client.get(reverse('analytics-topic-comparison'))
+        self.assertEqual(response.status_code, status.HTTP_200_OK)
+        data = response.data
+
+        self.assertEqual(len(data['by_topic']), 2)
+        self.assertEqual(len(data['by_course']), 1)
+        topic_minutes = {entry['topic_title']: entry['total_minutes'] for entry in data['by_topic']}
+        self.assertGreaterEqual(topic_minutes['Idade Média'], 45)
+        self.assertGreaterEqual(topic_minutes['Idade Moderna'], 120)
+        self.assertIn('summary', data)
+
+    def test_engagement_metrics_endpoint(self):
+        response = self.client.get(reverse('analytics-engagement-metrics'))
+        self.assertEqual(response.status_code, status.HTTP_200_OK)
+        data = response.data
+
+        self.assertGreaterEqual(data['current_streak'], 1)
+        self.assertGreaterEqual(data['best_streak'], data['current_streak'])
+        self.assertGreater(data['total_minutes_last_7_days'], 0)
+        self.assertGreaterEqual(len(data['weekly_minutes']), 1)
+        self.assertIsNotNone(data['summary'])
+
+    def test_dashboard_endpoint_combines_sections(self):
+        response = self.client.get(reverse('analytics-dashboard'))
+        self.assertEqual(response.status_code, status.HTTP_200_OK)
+        data = response.data
+
+        self.assertIn('study_effectiveness', data)
+        self.assertIn('score_progression', data)
+        self.assertIn('topic_comparison', data)
+        self.assertIn('engagement_metrics', data)
\ No newline at end of file
diff --git a/apps/analytics/urls.py b/apps/analytics/urls.py
index 043ffa79d7f5b0cabc62bc2cdc2ac470b047e5ea..7af3807bb71eac14058286dc7320f754d4e9ba69 100644
--- a/apps/analytics/urls.py
+++ b/apps/analytics/urls.py
@@ -1,9 +1,35 @@
 # apps/analytics/urls.py
 
 from django.urls import path
-from .views import StudyEffectivenessAPIView
+
+from .views import (
+    AnalyticsDashboardAPIView,
+    EngagementMetricsAPIView,
+    StudyEffectivenessAPIView,
+    StudyProgressAPIView,
+    TopicComparisonAPIView,
+)
 
 urlpatterns = [
-    # Endpoint único para a análise de eficácia do estudo
-    path('study-effectiveness/', StudyEffectivenessAPIView.as_view(), name='analytics-study-effectiveness'),
+    path(
+        'study-effectiveness/',
+        StudyEffectivenessAPIView.as_view(),
+        name='analytics-study-effectiveness',
+    ),
+    path(
+        'study-progress/',
+        StudyProgressAPIView.as_view(),
+        name='analytics-study-progress',
+    ),
+    path(
+        'topic-comparison/',
+        TopicComparisonAPIView.as_view(),
+        name='analytics-topic-comparison',
+    ),
+    path(
+        'engagement-metrics/',
+        EngagementMetricsAPIView.as_view(),
+        name='analytics-engagement-metrics',
+    ),
+    path('dashboard/', AnalyticsDashboardAPIView.as_view(), name='analytics-dashboard'),
 ]
diff --git a/apps/analytics/views.py b/apps/analytics/views.py
index f49e9d62c4ac2ba693da286e89fa630b5b9dd3d1..c29ea599a374bb1d1b1f3ce41d1f7bb971712a4c 100644
--- a/apps/analytics/views.py
+++ b/apps/analytics/views.py
@@ -1,107 +1,488 @@
-from rest_framework.views import APIView
-from rest_framework.response import Response
-from rest_framework.permissions import IsAuthenticated
-from django.db.models import Sum, Avg
+from collections import defaultdict
+from datetime import timedelta
+
 import pandas as pd
+from django.db.models import Avg, Count, Sum
+from django.utils import timezone
+from rest_framework.permissions import IsAuthenticated
+from rest_framework.response import Response
+from rest_framework.views import APIView
 from scipy.stats import pearsonr
 
-from apps.learning.models import Topic
-from .serializers import CorrelationAnalyticsSerializer
+from apps.assessment.models import Attempt
+from apps.learning.models import Course, Topic
+from apps.scheduling.models import StudyLog
+
+from .serializers import (
+    AnalyticsDashboardSerializer,
+    CorrelationAnalyticsSerializer,
+    EngagementMetricsSerializer,
+    ScoreProgressSerializer,
+    TopicComparisonSerializer,
+)
 
 
 def get_correlation_interpretation(coefficient):
     """Função auxiliar para traduzir o valor numérico em texto."""
     if coefficient is None or pd.isna(coefficient):
         return "Não há dados suficientes para calcular a correlação."
     
     abs_coeff = abs(coefficient)
     
     if abs_coeff >= 0.7:
         strength = "forte"
     elif abs_coeff >= 0.4:
         strength = "moderada"
     elif abs_coeff >= 0.1:
         strength = "fraca"
     else:
         return "Não há correlação significativa entre o tempo de estudo e as notas."
 
     direction = "positiva" if coefficient > 0 else "negativa"
     
     if direction == "positiva":
         return f"Existe uma correlação {strength} e {direction}. Isso sugere que, em geral, quanto mais tempo você estuda um tópico, melhores tendem a ser suas notas nos quizzes."
     else:
         return f"Existe uma correlação {strength} e {direction}. Isso é incomum e pode indicar que o tempo de estudo não está sendo eficaz ou que os quizzes estão avaliando outros conhecimentos."
 
 
-class StudyEffectivenessAPIView(APIView):
-    """
-    Endpoint de análise que calcula a correlação entre o tempo de estudo
-    por tópico e as notas médias dos quizzes para esses tópicos.
-    URL: GET /api/analytics/study-effectiveness/
-    """
-    permission_classes = [IsAuthenticated]
+def calculate_study_effectiveness(user):
+    """Calcula a correlação entre tempo de estudo e desempenho em quizzes."""
+    topics_with_study = Topic.objects.filter(
+        course__user=user,
+        study_logs__isnull=False,
+    ).annotate(
+        total_minutes_studied=Sum('study_logs__minutes_studied')
+    ).distinct()
 
-    def get(self, request, *args, **kwargs):
-        # Atribuir o usuário autenticado como um atributo de instância
-        self.user = request.user
-
-        # 1. Calcular agregações separadamente para evitar JOINs múltiplos
-        topics_with_study = Topic.objects.filter(
-            course__user=self.user,
-            study_logs__isnull=False
-        ).annotate(
-            total_minutes_studied=Sum('study_logs__minutes_studied')
-        ).distinct()
-
-        topics_with_quiz = Topic.objects.filter(
-            course__user=self.user,
-            quizzes__isnull=False,
-            quizzes__attempts__isnull=False  # Garante que haja tentativas registradas
-        ).annotate(
-            average_quiz_score=Avg('quizzes__attempts__score')  # Calcula a média das pontuações das tentativas
-        ).distinct()
-
-        # 2. Combinar os dados manualmente - CORREÇÃO AQUI
-        analysis_data = []
-        for topic in topics_with_study:
-            # Encontrar dados de quiz para este tópico usando o queryset correto
-            quiz_data = topics_with_quiz.filter(id=topic.id).first()  # topics_with_quiz é um queryset
-            if quiz_data and topic.total_minutes_studied and quiz_data.average_quiz_score:
-                analysis_data.append({
+    topics_with_quiz = Topic.objects.filter(
+        course__user=user,
+        quizzes__isnull=False,
+        quizzes__attempts__isnull=False,
+    ).annotate(
+        average_quiz_score=Avg('quizzes__attempts__score')
+    ).distinct()
+
+    analysis_data = []
+    for topic in topics_with_study:
+        quiz_data = topics_with_quiz.filter(id=topic.id).first()
+        if quiz_data and topic.total_minutes_studied and quiz_data.average_quiz_score:
+            analysis_data.append(
+                {
                     "topic_id": topic.id,
                     "topic_title": topic.title,
                     "total_minutes_studied": topic.total_minutes_studied,
-                    "average_quiz_score": round(quiz_data.average_quiz_score, 2)
-                })
-
-        # Se não tivermos pelo menos 2 pontos de dados, não podemos calcular a correlação
-        if len(analysis_data) < 2:
-            result = {
-                "correlation_coefficient": None,
-                "interpretation": "São necessários pelo menos dois tópicos com tempo de estudo e notas de quiz registrados para calcular a correlação.",
-                "data_points": len(analysis_data),
-                "topic_data": analysis_data
-            }
-            serializer = CorrelationAnalyticsSerializer(result)
-            return Response(serializer.data)
-
-        # 3. Usar Pandas e SciPy para o cálculo
-        df = pd.DataFrame(analysis_data)
-        
-        try:
-            correlation_coefficient, _ = pearsonr(df['total_minutes_studied'], df['average_quiz_score'])
-            if pd.isna(correlation_coefficient):
-                correlation_coefficient = None
-        except Exception:
+                    "average_quiz_score": round(quiz_data.average_quiz_score, 2),
+                }
+            )
+
+    if len(analysis_data) < 2:
+        return {
+            "correlation_coefficient": None,
+            "interpretation": "São necessários pelo menos dois tópicos com tempo de estudo e notas de quiz registrados para calcular a correlação.",
+            "data_points": len(analysis_data),
+            "topic_data": analysis_data,
+        }
+
+    df = pd.DataFrame(analysis_data)
+
+    try:
+        correlation_coefficient, _ = pearsonr(
+            df["total_minutes_studied"], df["average_quiz_score"]
+        )
+        if pd.isna(correlation_coefficient):
             correlation_coefficient = None
+    except Exception:
+        correlation_coefficient = None
+
+    return {
+        "correlation_coefficient": correlation_coefficient,
+        "interpretation": get_correlation_interpretation(correlation_coefficient),
+        "data_points": len(df),
+        "topic_data": df.to_dict("records"),
+    }
+
+
+def calculate_score_progression(user):
+    """Analisa a evolução temporal das notas dos quizzes do usuário."""
+    attempts = (
+        Attempt.objects.filter(user=user, quiz__topic__course__user=user)
+        .select_related("quiz__topic__course")
+        .order_by("completed_at")
+    )
+
+    if not attempts.exists():
+        return {
+            "total_attempts": 0,
+            "trend_summary": "Ainda não há tentativas suficientes para analisar a evolução das notas.",
+            "timeline": [],
+            "per_topic": [],
+        }
+
+    timeline_map = defaultdict(lambda: {"total": 0.0, "count": 0})
+    topic_attempts = defaultdict(list)
+
+    for attempt in attempts:
+        attempt_date = attempt.completed_at.date()
+        timeline_map[attempt_date]["total"] += attempt.score
+        timeline_map[attempt_date]["count"] += 1
+        topic_attempts[attempt.quiz.topic].append((attempt.completed_at, attempt.score))
+
+    timeline = []
+    for date_key in sorted(timeline_map.keys()):
+        total = timeline_map[date_key]["total"]
+        count = timeline_map[date_key]["count"]
+        timeline.append(
+            {
+                "date": date_key,
+                "average_score": round(total / count, 2),
+                "attempt_count": count,
+            }
+        )
+
+    first_avg = timeline[0]["average_score"]
+    last_avg = timeline[-1]["average_score"]
+    variation = round(last_avg - first_avg, 2)
+    if abs(variation) < 1:
+        trend_summary = (
+            f"A média geral permaneceu estável em {last_avg:.2f}% nas últimas tentativas."
+        )
+    elif variation > 0:
+        trend_summary = (
+            f"Suas notas médias evoluíram de {first_avg:.2f}% para {last_avg:.2f}% (+{variation:.2f} pontos)."
+        )
+    else:
+        trend_summary = (
+            f"As notas médias caíram de {first_avg:.2f}% para {last_avg:.2f}% ({variation:.2f} pontos)."
+        )
+
+    per_topic = []
+    for topic, entries in topic_attempts.items():
+        entries.sort(key=lambda item: item[0])
+        first_score = entries[0][1]
+        last_score = entries[-1][1]
+        score_change = round(last_score - first_score, 2) if len(entries) > 1 else 0.0
+        per_topic.append(
+            {
+                "topic_id": topic.id,
+                "topic_title": topic.title,
+                "course_title": topic.course.title,
+                "attempt_count": len(entries),
+                "latest_score": round(last_score, 2),
+                "score_change": score_change,
+            }
+        )
+
+    per_topic.sort(key=lambda item: item["score_change"], reverse=True)
+
+    return {
+        "total_attempts": attempts.count(),
+        "trend_summary": trend_summary,
+        "timeline": timeline,
+        "per_topic": per_topic,
+    }
+
+
+def calculate_topic_comparison(user):
+    """Compara desempenho e dedicação entre tópicos e disciplinas."""
+    topics = (
+        Topic.objects.filter(course__user=user)
+        .select_related("course")
+        .order_by("course__title", "title")
+    )
+
+    study_log_totals = {
+        entry["topic_id"]: entry
+        for entry in StudyLog.objects.filter(user=user, topic__isnull=False)
+        .values("topic_id")
+        .annotate(
+            total_minutes=Sum("minutes_studied"),
+            session_count=Count("id"),
+        )
+    }
+
+    attempt_totals = {
+        entry["quiz__topic_id"]: entry
+        for entry in Attempt.objects.filter(user=user, quiz__topic__course__user=user)
+        .values("quiz__topic_id")
+        .annotate(
+            average_score=Avg("score"),
+            attempt_count=Count("id"),
+        )
+    }
+
+    by_topic = []
+    for topic in topics:
+        log_data = study_log_totals.get(topic.id, {})
+        attempt_data = attempt_totals.get(topic.id, {})
+        by_topic.append(
+            {
+                "topic_id": topic.id,
+                "topic_title": topic.title,
+                "course_title": topic.course.title,
+                "total_minutes": int(log_data.get("total_minutes") or 0),
+                "session_count": int(log_data.get("session_count") or 0),
+                "average_score": (
+                    round(attempt_data.get("average_score"), 2)
+                    if attempt_data.get("average_score") is not None
+                    else None
+                ),
+                "attempt_count": int(attempt_data.get("attempt_count") or 0),
+            }
+        )
+
+    courses = Course.objects.filter(user=user).order_by("title")
+
+    study_log_by_course = {
+        entry["course_id"]: entry
+        for entry in StudyLog.objects.filter(user=user)
+        .values("course_id")
+        .annotate(
+            total_minutes=Sum("minutes_studied"),
+            session_count=Count("id"),
+        )
+    }
+
+    attempt_by_course = {
+        entry["quiz__topic__course_id"]: entry
+        for entry in Attempt.objects.filter(user=user, quiz__topic__course__user=user)
+        .values("quiz__topic__course_id")
+        .annotate(
+            average_score=Avg("score"),
+            attempt_count=Count("id"),
+        )
+    }
+
+    by_course = []
+    for course in courses:
+        log_data = study_log_by_course.get(course.id, {})
+        attempt_data = attempt_by_course.get(course.id, {})
+        by_course.append(
+            {
+                "course_id": course.id,
+                "course_title": course.title,
+                "total_minutes": int(log_data.get("total_minutes") or 0),
+                "session_count": int(log_data.get("session_count") or 0),
+                "average_score": (
+                    round(attempt_data.get("average_score"), 2)
+                    if attempt_data.get("average_score") is not None
+                    else None
+                ),
+                "attempt_count": int(attempt_data.get("attempt_count") or 0),
+            }
+        )
+
+    if by_topic:
+        best_topic = max(
+            by_topic,
+            key=lambda item: item["average_score"] or 0,
+        )
+        summary = (
+            f"O tópico com melhor média de desempenho é '{best_topic['topic_title']}' "
+            f"com {best_topic['average_score'] or 0:.2f}% após {best_topic['attempt_count']} tentativa(s)."
+        )
+    else:
+        summary = "Ainda não há dados suficientes para comparar tópicos ou disciplinas."
+
+    return {
+        "by_topic": by_topic,
+        "by_course": by_course,
+        "summary": summary,
+    }
+
 
-        # 4. Montar a resposta final
-        result = {
-            "correlation_coefficient": correlation_coefficient,
-            "interpretation": get_correlation_interpretation(correlation_coefficient),
-            "data_points": len(df),
-            "topic_data": df.to_dict('records')
+def calculate_engagement_metrics(user):
+    """Gera métricas de engajamento como streak e regularidade."""
+    logs_qs = StudyLog.objects.filter(user=user).order_by("date")
+    if not logs_qs.exists():
+        return {
+            "current_streak": 0,
+            "best_streak": 0,
+            "total_minutes_last_7_days": 0,
+            "total_minutes_last_30_days": 0,
+            "average_session_minutes": None,
+            "sessions_last_7_days": 0,
+            "regularity_score": 0.0,
+            "most_productive_day": None,
+            "weekly_minutes": [],
+            "summary": "Registre novas sessões de estudo para ver métricas de engajamento.",
         }
 
+    aggregate = logs_qs.aggregate(
+        total_minutes=Sum("minutes_studied"),
+        total_sessions=Count("id"),
+    )
+    total_minutes = aggregate["total_minutes"] or 0
+    total_sessions = aggregate["total_sessions"] or 0
+    average_session_minutes = (
+        round(total_minutes / total_sessions, 2) if total_sessions else None
+    )
+
+    daily_data = list(
+        logs_qs.values("date")
+        .annotate(
+            total_minutes=Sum("minutes_studied"),
+            session_count=Count("id"),
+        )
+        .order_by("date")
+    )
+
+    today = timezone.localdate()
+    seven_days_ago = today - timedelta(days=6)
+    thirty_days_ago = today - timedelta(days=29)
+    twenty_eight_days_ago = today - timedelta(days=27)
+
+    total_minutes_last_7_days = sum(
+        entry["total_minutes"]
+        for entry in daily_data
+        if entry["date"] >= seven_days_ago
+    )
+    total_minutes_last_30_days = sum(
+        entry["total_minutes"]
+        for entry in daily_data
+        if entry["date"] >= thirty_days_ago
+    )
+    sessions_last_7_days = sum(
+        entry["session_count"]
+        for entry in daily_data
+        if entry["date"] >= seven_days_ago
+    )
+
+    unique_dates = [entry["date"] for entry in daily_data]
+    date_set = set(unique_dates)
+
+    current_streak = 0
+    streak_day = today
+    while streak_day in date_set:
+        current_streak += 1
+        streak_day -= timedelta(days=1)
+
+    best_streak = 0
+    temp_streak = 0
+    previous_date = None
+    for study_date in unique_dates:
+        if previous_date and study_date == previous_date + timedelta(days=1):
+            temp_streak += 1
+        else:
+            temp_streak = 1
+        best_streak = max(best_streak, temp_streak)
+        previous_date = study_date
+
+    days_last_28 = sum(1 for d in unique_dates if d >= twenty_eight_days_ago)
+    regularity_score = round((days_last_28 / 28) * 100, 2)
+
+    weekday_labels = [
+        "Segunda-feira",
+        "Terça-feira",
+        "Quarta-feira",
+        "Quinta-feira",
+        "Sexta-feira",
+        "Sábado",
+        "Domingo",
+    ]
+    weekday_minutes = defaultdict(int)
+    for entry in daily_data:
+        weekday_minutes[entry["date"].weekday()] += entry["total_minutes"]
+
+    most_productive_day = None
+    if weekday_minutes:
+        best_weekday = max(weekday_minutes, key=weekday_minutes.get)
+        most_productive_day = weekday_labels[best_weekday]
+
+    weekly_minutes = []
+    for week_index in range(4):
+        week_end = today - timedelta(days=week_index * 7)
+        week_start = week_end - timedelta(days=6)
+        total = sum(
+            entry["total_minutes"]
+            for entry in daily_data
+            if week_start <= entry["date"] <= week_end
+        )
+        weekly_minutes.append(
+            {
+                "week_start": week_start,
+                "week_end": week_end,
+                "total_minutes": total,
+            }
+        )
+
+    weekly_minutes.sort(key=lambda item: item["week_start"])
+
+    summary = (
+        f"Você estudou {sessions_last_7_days} vez(es) nos últimos 7 dias, "
+        f"acumulando {total_minutes_last_7_days} minutos. "
+        f"Sequência atual: {current_streak} dia(s); melhor sequência: {best_streak} dia(s)."
+    )
+
+    return {
+        "current_streak": current_streak,
+        "best_streak": best_streak,
+        "total_minutes_last_7_days": total_minutes_last_7_days,
+        "total_minutes_last_30_days": total_minutes_last_30_days,
+        "average_session_minutes": average_session_minutes,
+        "sessions_last_7_days": sessions_last_7_days,
+        "regularity_score": regularity_score,
+        "most_productive_day": most_productive_day,
+        "weekly_minutes": weekly_minutes,
+        "summary": summary,
+    }
+
+
+class StudyEffectivenessAPIView(APIView):
+    """Calcula a correlação entre minutos estudados e notas médias."""
+
+    permission_classes = [IsAuthenticated]
+
+    def get(self, request, *args, **kwargs):
+        result = calculate_study_effectiveness(request.user)
         serializer = CorrelationAnalyticsSerializer(result)
+        return Response(serializer.data)
+
+
+class StudyProgressAPIView(APIView):
+    """Retorna a evolução temporal das notas em quizzes."""
+
+    permission_classes = [IsAuthenticated]
+
+    def get(self, request, *args, **kwargs):
+        result = calculate_score_progression(request.user)
+        serializer = ScoreProgressSerializer(result)
+        return Response(serializer.data)
+
+
+class TopicComparisonAPIView(APIView):
+    """Compara dedicação e desempenho entre tópicos e cursos."""
+
+    permission_classes = [IsAuthenticated]
+
+    def get(self, request, *args, **kwargs):
+        result = calculate_topic_comparison(request.user)
+        serializer = TopicComparisonSerializer(result)
+        return Response(serializer.data)
+
+
+class EngagementMetricsAPIView(APIView):
+    """Entrega métricas de engajamento e hábitos de estudo."""
+
+    permission_classes = [IsAuthenticated]
+
+    def get(self, request, *args, **kwargs):
+        result = calculate_engagement_metrics(request.user)
+        serializer = EngagementMetricsSerializer(result)
+        return Response(serializer.data)
+
+
+class AnalyticsDashboardAPIView(APIView):
+    """Resumo consolidado com as principais análises de estudo."""
+
+    permission_classes = [IsAuthenticated]
+
+    def get(self, request, *args, **kwargs):
+        payload = {
+            "study_effectiveness": calculate_study_effectiveness(request.user),
+            "score_progression": calculate_score_progression(request.user),
+            "topic_comparison": calculate_topic_comparison(request.user),
+            "engagement_metrics": calculate_engagement_metrics(request.user),
+        }
+        serializer = AnalyticsDashboardSerializer(payload)
         return Response(serializer.data)
\ No newline at end of file
